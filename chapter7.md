Classification
features = ลักษณะ แทนด้วย x
target = เป้าหมาย แทนด้วย y
 
1. KNN บทที่ 9 (Lazy learner เพราะ มันไม่ต้องสร้างโมเดล)  K เท่ากับ 3 คือเอาตัวใกล้สุด 3 ตัว แล้วโหวตว่าคำตอบไหนเยอะกว่า กำหนดเป็นเลขคี่ดีกว่าจะได้ไม่โหวตเท่ากัน     
Data > Train + Test > K-fold cross validation (k=3) Train จากขั้นตอนก่อนหน้า จะถูกแบ่งเป็น 3 ส่วน รอบที่ 1 Train1 เป็น Test Train2,3 เป็น Train รอบ 2 เปลี่ยน Train2 เป็น Test ที่เหลือเป็น train และ รอบที่ 3 เหมือนเดิม สุดท้ายเอาโมเดลเดียวกันมาเทียบกัน อันไหนดีสุดก็เอาอันนั้น สุดท้ายเอา Train รวมกันอีกและ Train และ Test เดิมมาใช้ ละก็จะได้ความแม่นยำออกมา
2. Decision tree มันจะดีมากเมื่อมี feature เยอะๆ
root node คือ หัวบนสุด leaf node คือ ปลายสุด 
การสร้างต้นไม้ ดู gain ของ feature ว่า feature ไหนแบ่งข้อมูลได้ดีที่สุด คือ gain สูงที่สุด เลือก feature ที่แบ่ง Data ได้ที่สุด ให้กำหนดเป็น root node ละก็ทำไปเรื่อยๆ จน feature หมด
Info(D) = I(9.5) คือ yes 9 No 5 = -9/14(log2(9/14))-5/14(log(5/14))
Infoage(D) = แบ่งเป็น <30, 31-40, >40 ถ้าเป็น node อื่นก็แบ่งตามนั้น ถ้าอันที่ดีคือ (4,0) คือมีค่า 0 ด้วย
แบบยาก > ต้นไม้หยุดโตเมื่อ 1. feature หมดแล้ว ละยังแบ่งไม่จบ ให้ตอบแบบเป็นเปอร์เซ็นว่าน่าจะเป็น class นี้กี่เปอร์เซ็น  class นี้กี่เปอร์เซ็น 2. ตัวอย่างมี class เดียวกันหมด  3. ไม่มีตัวอย่างเหลือแล้ว
การจัดการกับข้อมูลที่เป็น continuous 1. เรียงลำดับข้อมูล 2. หาจุดแบ่งโดยการ คำนวณ Info(D), Infofx(D) = ส่วนแรก + ส่วนหลัง
3. Logistic regression
4. Neural network
